#flume配置
#将kafka数据推送到hdfs

# 定义agent
a1.sources = s1
a1.channels = c1
a1.sinks = k1

#定义source
a1.sources.s1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.s1.channels = memoryChannel
a1.sources.s1.batchSize = 5000
a1.sources.s1.batchDurationMillis = 2000
a1.sources.s1.kafka.bootstrap.servers = localhost:9092
a1.sources.s1.kafka.topics = topic_flume_sys_trace_data
a1.sources.s1.kafka.consumer.group.id = flume
a1.sources.s1.kafka.consumer.timeout.ms = 100


#定义channel
a1.channels.c1.type=memory
a1.channels.c1.capacity=10000
a1.channels.c1.transactionCapacity=100

#定义sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = memoryChannel
a1.sinks.k1.hdfs.path = /flume/logs/big_data_server/%Y%m%d/%H
a1.sinks.k1.hdfs.filePrefix = events_%Y_%m_%d
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 0

# 绑定source, sink到channel
a1.sources.s1.channels=c1
a1.sinks.k1.channel = c1

#cd /opt/flume/apache-flume-1.9.0-bin/
#./bin/flume-ng agent -n a1 -f $FLUME_HOME/conf/kafka_to_hdfs.conf &