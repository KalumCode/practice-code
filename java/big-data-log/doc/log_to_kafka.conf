#flume配置
#把log日志推送到kafka

# 定义agent
a1.sources = s1
a1.channels = c1
a1.sinks = k1

#定义source
a1.sources.s1.type=exec
a1.sources.s1.command=tail -F /opt/backend/big-data-log/logs/sys_trace.log

#定义channel
a1.channels.c1.type=memory
a1.channels.c1.capacity=10000
a1.channels.c1.transactionCapacity=100

#定义sink (Kafka)
#设置Kafka接收器
a1.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink
#设置Kafka的broker地址和端口号
a1.sinks.k1.brokerList=192.168.0.128:9092
#设置Kafka的Topic
a1.sinks.k1.topic=topic_flume_sys_trace_data
#设置序列化方式
a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder

# 绑定source, sink到channel
a1.sources.s1.channels=c1
a1.sinks.k1.channel = c1

#cd /opt/flume/apache-flume-1.9.0-bin/
#./bin/flume-ng agent --name a1 --conf conf/ --conf-file conf/backend/big_data_log_kafka_logger.conf -Dflume.root.logger=INFO,console